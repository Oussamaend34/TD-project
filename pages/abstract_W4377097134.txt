Random forest is one of the most used machine learning algorithms since its high predictive performance. However, many studies criticize it for the fact that it generates a large number of trees, which requires important storage space and a significant learning time. In addition, the final model induced by RF may contain redundant trees and others that do not contribute to the prediction that may even disadvantage performance. This is why many researchers try to reduce the number of trees in a forest called forest pruning. This article presents a study of the pruning work of random forest classifiers, explains in detail the operating principle of each technique, and cites their advantages and disadvantages. Finally, it compares their classification performance in terms of accuracy, speed of learning, and complexity.